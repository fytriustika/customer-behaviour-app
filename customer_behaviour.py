# -*- coding: utf-8 -*-
"""customer_behaviour.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QtGfH37oihG6PsXgG6MHuS-AL5Tgr8_2
"""

from google.colab import drive
drive.mount('/content/drive')

import os

# Replace this path with the path to the directory where your files are located
directory_path = '/content/drive/MyDrive/e-commerce customer behaviour dataset/'

# List the contents of the directory
try:
    contents = os.listdir(directory_path)
    print(f"Contents of '{directory_path}':")
    for item in contents:
        print(item)
except FileNotFoundError:
    print(f"Error: The directory '{directory_path}' was not found.")
except Exception as e:
    print(f"An error occurred: {e}")

import pandas as pd
import os

directory_path = '/content/drive/MyDrive/e-commerce customer behaviour dataset/'

df1 = pd.read_excel(os.path.join(directory_path, 'Online_Retail_Cleaned.xlsx'))
df2 = pd.read_csv(os.path.join(directory_path, 'rfm_segmented.csv'))

print("Jumlah data (baris, kolom):", df1.shape)
print("Jumlah adata (baris, kolom):", df2.shape)
print("\nInformasi tipe data dan skema pengkodean:")
print(df1.info())
print(df2.info())
print("\nStatistik deskriptif untuk data numerik:")
print(df1.describe())
print(df2.describe())
print("\nBeberapa baris pertama data:")
print(df1.head())
print(df2.head())
print("\nJumlah nilai unik untuk setiap kolom:")
for column in df1.columns:
    print(f"{column}: {df1[column].nunique()}")

for column in df2.columns:
    print(f"{column}:{df2[column].nunique()}")

print("\nJumlah nilai yang hilang per kolom:")
print(df1.isnull().sum())
print(df2.isnull().sum())

for col in df1.select_dtypes(include=['object']).columns:
    print(f"\nNilai unik di kolom '{col}' (untuk memeriksa potensi data yang salah):")
    print(df1[col].unique()[:50])

for col in df2.select_dtypes(include=['object']).columns:
    print(f"\nNilai unik di kolom '{col}' (untuk memriksa potensi data yang salah):")
    print(df2[col].unique()[:50])

print("\nMemeriksa potensi outlier melalui rentang nilai dalam statistik deskriptif:")
print(df1.describe())
print(df2.describe())

# Drop rows with missing values in any of the columns in df1.
df1.dropna(inplace=True)

# Convert the Quantity and UnitPrice columns to numeric types, coercing errors.
df1['Quantity'] = pd.to_numeric(df1['Quantity'], errors='coerce')
df1['UnitPrice'] = pd.to_numeric(df1['UnitPrice'], errors='coerce')

# Remove rows where Quantity is less than or equal to 0 or UnitPrice is less than or equal to 0 in df1.
df1 = df1[(df1['Quantity'] > 0) & (df1['UnitPrice'] > 0)]

# Remove rows where CustomerID is missing in df1 (already handled by dropna, but good for explicit check).
df1.dropna(subset=['CustomerID'], inplace=True)

# Convert the CustomerID column to an integer type.
df1['CustomerID'] = df1['CustomerID'].astype(int)

# Display the first few rows and info of the cleaned df1
display(df1.head())
display(df1.info())

"""**Reasoning**:
The previous step successfully cleaned `df1` by handling missing values, converting data types, and filtering based on quantity and unit price. Now, I need to check for outliers in the numerical columns of `df1` and `df2` using descriptive statistics and visualize them.


"""

# Check for outliers in numerical columns of df1 using descriptive statistics
print("Descriptive statistics for df1 after cleaning:")
display(df1.describe())

# Check for outliers in numerical columns of df2 using descriptive statistics
print("\nDescriptive statistics for df2:")
display(df2.describe())

import matplotlib.pyplot as plt
import seaborn as sns

# Select numerical columns for box plots in df1
numerical_cols_df1 = df1.select_dtypes(include=['float64', 'int64']).columns

# Create box plots for numerical columns in df1
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols_df1):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(y=df1[col])
    plt.title(f'Box Plot of {col} in df1')
plt.tight_layout()
plt.show()

# Select numerical columns for box plots in df2
numerical_cols_df2 = df2.select_dtypes(include=['float64', 'int64']).columns

# Create box plots for numerical columns in df2
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols_df2):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(y=df2[col])
    plt.title(f'Box Plot of {col} in df2')
plt.tight_layout()
plt.show()

"""**Reasoning**:
The descriptive statistics and box plots show potential outliers in several numerical columns in both dataframes. Based on the box plots, columns like Quantity, UnitPrice, TotalPrice, Recency, Frequency, and Monetary appear to have outliers. I will use the IQR method to remove outliers from these columns in both dataframes.


"""

# Function to remove outliers using IQR
def remove_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_cleaned = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy()
    return df_cleaned

# Remove outliers from relevant columns in df1
df1_cleaned = remove_outliers_iqr(df1, 'Quantity')
df1_cleaned = remove_outliers_iqr(df1_cleaned, 'UnitPrice')
df1_cleaned = remove_outliers_iqr(df1_cleaned, 'TotalPrice')

# Remove outliers from relevant columns in df2
df2_cleaned = remove_outliers_iqr(df2, 'Recency')
df2_cleaned = remove_outliers_iqr(df2_cleaned, 'Frequency')
df2_cleaned = remove_outliers_iqr(df2_cleaned, 'Monetary')


# Display the shape of the dataframes after outlier removal
print("Shape of df1 after outlier removal:", df1_cleaned.shape)
print("Shape of df2 after outlier removal:", df2_cleaned.shape)

# Display descriptive statistics after outlier removal
print("\nDescriptive statistics for df1 after outlier removal:")
display(df1_cleaned.describe())

print("\nDescriptive statistics for df2 after outlier removal:")
display(df2_cleaned.describe())

"""**Reasoning**:
Check for missing values in df2 and then check the data types of df2 to determine if any type conversions are needed.


"""

# Check for missing values in df2
print("Jumlah nilai yang hilang per kolom di df2:")
print(df2.isnull().sum())

# Check data types of df2
print("\nInformasi tipe data dan skema pengkodean di df2:")
print(df2.info())

"""**Reasoning**:
Perform data validation by checking the range of numerical columns, confirming date formats, verifying unique customer IDs, examining categorical values, and checking for duplicate rows in both dataframes.


"""

# 1. Periksa rentang nilai untuk kolom numerik
print("Rentang nilai untuk kolom numerik di df1_cleaned:")
display(df1_cleaned.describe())

print("\nRentang nilai untuk kolom numerik di df2_cleaned:")
display(df2_cleaned.describe())

# 2. Validasi format data untuk kolom tanggal di df1_cleaned
print("\nInformasi tipe data dan skema pengkodean di df1_cleaned:")
df1_cleaned.info()

# 3. Periksa keunikan nilai di kolom CustomerID di df2_cleaned
print(f"\nJumlah nilai unik di kolom 'CustomerID' di df2_cleaned: {df2_cleaned['CustomerID'].nunique()}")
print(f"Jumlah baris di df2_cleaned: {df2_cleaned.shape[0]}")

# 4. Validasi nilai dalam kolom kategorikal
print("\nNilai unik di kolom 'Country' di df1_cleaned (untuk memeriksa potensi data yang salah):")
print(df1_cleaned['Country'].unique())

print("\nNilai unik di kolom 'Segment' di df2_cleaned (untuk memriksa potensi data yang salah):")
print(df2_cleaned['Segment'].unique())

# 5. Periksa apakah ada duplikasi baris
print(f"\nJumlah duplikasi baris di df1_cleaned: {df1_cleaned.duplicated().sum()}")
print(f"Jumlah duplikasi baris di df2_cleaned: {df2_cleaned.duplicated().sum()}")

"""**Reasoning**:
Based on the validation results, there are duplicate rows in `df1_cleaned` that need to be removed. `df2_cleaned` has no duplicates and the validation checks passed for both dataframes. Therefore, I will remove the duplicate rows from `df1_cleaned`.


"""

# Remove duplicate rows from df1_cleaned
df1_cleaned = df1_cleaned.drop_duplicates()

# Verify that duplicates have been removed
print(f"Jumlah duplikasi baris di df1_cleaned setelah dihapus: {df1_cleaned.duplicated().sum()}")

"""**Reasoning**:
Identify and describe the main data objects and their relevant attributes based on the cleaned dataframes.


"""

# Identify and describe the main data objects based on df1_cleaned and df2_cleaned

# Object 1: Pelanggan (Customer)
# Attributes:
#   CustomerID: Unique identifier for each customer. (from df1_cleaned and df2_cleaned)
#   Country: Country of the customer. (from df1_cleaned)
#   Recency: Days since the last purchase. (from df2_cleaned)
#   Frequency: Total number of purchases. (from df2_cleaned)
#   Monetary: Total amount spent. (from df2_cleaned)
#   R_score, F_score, M_score: RFM scores for Recency, Frequency, and Monetary. (from df2_cleaned)
#   RFM_Score: Combined RFM score. (from df2_cleaned)
#   Segment: Customer segment based on RFM analysis. (from df2_cleaned)

# Object 2: Produk (Product)
# Attributes:
#   StockCode: Unique identifier for each product. (from df1_cleaned)
#   Description: Description of the product. (from df1_cleaned)
#   UnitPrice: Price per unit of the product. (from df1_cleaned)

# Object 3: Transaksi (Transaction/Order Line Item)
# Attributes:
#   InvoiceNo: Unique identifier for each invoice/transaction. (from df1_cleaned)
#   StockCode: Identifier for the product in the transaction. (from df1_cleaned)
#   Quantity: Quantity of the product in the transaction. (from df1_cleaned)
#   InvoiceDate: Date and time of the transaction. (from df1_cleaned)
#   UnitPrice: Price per unit of the product in the transaction. (from df1_cleaned)
#   CustomerID: Identifier for the customer who made the transaction. (from df1_cleaned)
#   Country: Country where the transaction occurred. (from df1_cleaned)
#   TotalPrice: Total price for this specific item in the transaction (Quantity * UnitPrice). (from df1_cleaned)

print("Identifikasi Objek Data Utama dan Atributnya:")
print("\nObjek: Pelanggan (Customer)")
print("- CustomerID: Pengenal unik untuk setiap pelanggan.")
print("- Country: Negara tempat pelanggan berada.")
print("- Recency: Jumlah hari sejak pembelian terakhir.")
print("- Frequency: Jumlah total pembelian yang dilakukan.")
print("- Monetary: Jumlah total uang yang dihabiskan oleh pelanggan.")
print("- R_score, F_score, M_score: Skor RFM untuk Recency, Frequency, dan Monetary.")
print("- RFM_Score: Skor RFM gabungan.")
print("- Segment: Segmen pelanggan berdasarkan analisis RFM.")

print("\nObjek: Produk (Product)")
print("- StockCode: Pengenal unik untuk setiap produk.")
print("- Description: Deskripsi produk.")
print("- UnitPrice: Harga per unit produk.")

print("\nObjek: Transaksi (Transaction/Order Line Item)")
print("- InvoiceNo: Pengenal unik untuk setiap faktur/transaksi.")
print("- StockCode: Pengenal produk dalam transaksi.")
print("- Quantity: Jumlah produk dalam transaksi.")
print("- InvoiceDate: Tanggal dan waktu transaksi.")
print("- UnitPrice: Harga per unit produk dalam transaksi (pada saat transaksi).")
print("- CustomerID: Pengenal pelanggan yang melakukan transaksi.")
print("- Country: Negara tempat transaksi terjadi.")
print("- TotalPrice: Total harga untuk item spesifik dalam transaksi (Quantity * UnitPrice).")

"""**Reasoning**:
Calculate Recency, Frequency, and Monetary for each customer and combine them into a new DataFrame. This addresses steps 1-6 of the subtask.


"""

import datetime as dt

# Step 1: Calculate the last transaction date for each CustomerID
last_purchase_date = df1_cleaned.groupby('CustomerID')['InvoiceDate'].max().reset_index()

# Step 2: Determine a reference date (one day after the last transaction date in the dataset)
reference_date = df1_cleaned['InvoiceDate'].max() + dt.timedelta(days=1)

# Step 3: Calculate Recency for each customer
last_purchase_date['Recency'] = (reference_date - last_purchase_date['InvoiceDate']).dt.days

# Step 4: Calculate Frequency for each customer
frequency_df = df1_cleaned.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()
frequency_df.rename(columns={'InvoiceNo': 'Frequency'}, inplace=True)

# Step 5: Calculate Monetary for each customer
monetary_df = df1_cleaned.groupby('CustomerID')['TotalPrice'].sum().reset_index()
monetary_df.rename(columns={'TotalPrice': 'Monetary'}, inplace=True)

# Step 6: Merge the results into a new DataFrame indexed by CustomerID
rfm_df = last_purchase_date[['CustomerID', 'Recency']].merge(frequency_df, on='CustomerID')
rfm_df = rfm_df.merge(monetary_df, on='CustomerID')

# Display the first few rows of the new RFM DataFrame
display(rfm_df.head())

"""**Reasoning**:
I will define a function to assign scores for Recency, Frequency, and Monetary using quantiles, apply this function to create score columns in `rfm_df`, combine these scores into a composite RFM score, define rules for customer segmentation based on the RFM scores, apply these rules to create a 'Segment' column, and finally display the count of customers in each segment. This covers steps 1 through 6 of the current subtask.


"""

# Step 1 & 2: Define and apply scoring function using quantiles
def score_rfm(df, column, is_recency=False):
    if is_recency:
        # For Recency, lower values are better, so we reverse the scoring
        return pd.qcut(df[column], 5, labels=[5, 4, 3, 2, 1])
    else:
        # For Frequency and Monetary, higher values are better
        return pd.qcut(df[column], 5, labels=[1, 2, 3, 4, 5])

rfm_df['R_score'] = score_rfm(rfm_df, 'Recency', is_recency=True)
rfm_df['F_score'] = score_rfm(rfm_df, 'Frequency', is_recency=False)
rfm_df['M_score'] = score_rfm(rfm_df, 'Monetary', is_recency=False)

# Step 3: Combine R, F, and M scores to create a composite RFM score
rfm_df['RFM_Score'] = rfm_df['R_score'].astype(str) + rfm_df['F_score'].astype(str) + rfm_df['M_score'].astype(str)
rfm_df['RFM_Score'] = rfm_df['RFM_Score'].astype(int)


# Step 4 & 5: Define and apply segmentation rules
def segment_customer(rfm_score):
    if rfm_score >= 555:
        return 'Best Customers'
    elif rfm_score >= 454:
        return 'Loyal Customers'
    elif rfm_score >= 414:
        return 'Recent but Infrequent'
    elif rfm_score >= 333:
        return 'Promising'
    elif rfm_score >= 222:
        return 'At Risk'
    elif rfm_score >= 111:
        return 'Lost Customers'
    else:
        return 'Others' # Handle any unexpected scores

rfm_df['Segment'] = rfm_df['RFM_Score'].apply(segment_customer)

# Step 6: Display the count of customers in each segment
print("Number of customers in each segment:")
display(rfm_df['Segment'].value_counts())

# Display the first few rows of the rfm_df with scores and segment
display(rfm_df.head())

"""**Reasoning**:
The previous code failed because `pd.qcut` encountered non-unique bin edges for the 'Frequency' column. This happens when there are many identical values in the column, especially at the lower quantiles, causing the quantile boundaries to be the same. To fix this, I need to set the `duplicates='drop'` argument in `pd.qcut` for the Frequency and Monetary columns to handle these cases by dropping the non-unique bin edges. I will then re-run the scoring, RFM score calculation, segmentation, and display the results, completing all steps of the subtask.


"""

# Step 1 & 2: Define and apply scoring function using quantiles, handling duplicates and adjusting labels
def score_rfm(df, column, is_recency=False):
    # Use qcut with duplicates='drop', and get the bins and labels (as integers initially)
    scored_data, bins = pd.qcut(df[column], 5, labels=False, retbins=True, duplicates='drop')

    # Determine the number of unique bins created
    num_unique_bins = len(bins) - 1

    # Create labels based on the number of unique bins
    if is_recency:
        labels = list(range(num_unique_bins, 0, -1))
    else:
        labels = list(range(1, num_unique_bins + 1))

    # Map the integer labels from qcut to the desired score labels
    score_map = {i: labels[i] for i in range(num_unique_bins)}
    final_scores = scored_data.map(score_map)

    return final_scores

rfm_df['R_score'] = score_rfm(rfm_df, 'Recency', is_recency=True)
rfm_df['F_score'] = score_rfm(rfm_df, 'Frequency', is_recency=False)
rfm_df['M_score'] = score_rfm(rfm_df, 'Monetary', is_recency=False)

# Step 3: Combine R, F, and M scores to create a composite RFM score
# Ensure scores are treated as strings before concatenation
rfm_df['RFM_Score'] = rfm_df['R_score'].astype(str) + rfm_df['F_score'].astype(str) + rfm_df['M_score'].astype(str)
rfm_df['RFM_Score'] = rfm_df['RFM_Score'].astype(int)

# Step 4 & 5: Define and apply segmentation rules
def segment_customer(rfm_score):
    if rfm_score >= 555:
        return 'Best Customers'
    elif rfm_score >= 454:
        return 'Loyal Customers'
    elif rfm_score >= 414:
        return 'Recent but Infrequent'
    elif rfm_score >= 333:
        return 'Promising'
    elif rfm_score >= 222:
        return 'At Risk'
    elif rfm_score >= 111:
        return 'Lost Customers'
    else:
        return 'Others' # Handle any unexpected scores

rfm_df['Segment'] = rfm_df['RFM_Score'].apply(segment_customer)

# Step 6: Display the count of customers in each segment
print("Number of customers in each segment:")
display(rfm_df['Segment'].value_counts())

# Display the first few rows of the rfm_df with scores and segment
display(rfm_df.head())

"""**Reasoning**:
The error "Bin labels must be one fewer than the number of bin edges" indicates that after dropping duplicate bin edges, the number of resulting bins is less than the number of labels provided (which is 5). This means that even with `duplicates='drop'`, there are still too many identical values preventing the creation of 5 distinct quantiles. To address this, I need to dynamically determine the number of unique bins created by `qcut` when `duplicates='drop'` is used and adjust the number of labels accordingly. I will modify the `score_rfm` function to handle this by checking the number of unique bins generated and creating labels based on that number. Then I will re-run the scoring, RFM score calculation, segmentation, and display the results, completing all steps of the subtask.


"""

# Step 1 & 2: Define and apply scoring function using quantiles, handling duplicates and adjusting labels
def score_rfm(df, column, is_recency=False):
    # Use qcut with duplicates='drop'
    scored_data, bins = pd.qcut(df[column], 5, labels=False, retbins=True, duplicates='drop')

    # Determine the number of unique bins created
    num_unique_bins = len(bins) - 1

    # Create labels based on the number of unique bins
    if is_recency:
        labels = list(range(num_unique_bins, 0, -1))
    else:
        labels = list(range(1, num_unique_bins + 1))

    # Apply the labels to the scored data
    # Need to map the integer labels from qcut to the desired score labels
    score_map = {i: labels[i] for i in range(num_unique_bins)}
    final_scores = scored_data.map(score_map)

    return final_scores

rfm_df['R_score'] = score_rfm(rfm_df, 'Recency', is_recency=True)
rfm_df['F_score'] = score_rfm(rfm_df, 'Frequency', is_recency=False)
rfm_df['M_score'] = score_rfm(rfm_df, 'Monetary', is_recency=False)

# Step 3: Combine R, F, and M scores to create a composite RFM score
# Ensure scores are treated as strings before concatenation
rfm_df['RFM_Score'] = rfm_df['R_score'].astype(str) + rfm_df['F_score'].astype(str) + rfm_df['M_score'].astype(str)
rfm_df['RFM_Score'] = rfm_df['RFM_Score'].astype(int)

# Step 4 & 5: Define and apply segmentation rules
def segment_customer(rfm_score):
    if rfm_score >= 555:
        return 'Best Customers'
    elif rfm_score >= 454:
        return 'Loyal Customers'
    elif rfm_score >= 414:
        return 'Recent but Infrequent'
    elif rfm_score >= 333:
        return 'Promising'
    elif rfm_score >= 222:
        return 'At Risk'
    elif rfm_score >= 111:
        return 'Lost Customers'
    else:
        return 'Others' # Handle any unexpected scores

rfm_df['Segment'] = rfm_df['RFM_Score'].apply(segment_customer)

# Step 6: Display the count of customers in each segment
print("Number of customers in each segment:")
display(rfm_df['Segment'].value_counts())

# Display the first few rows of the rfm_df with scores and segment
display(rfm_df.head())

"""**Reasoning**:
Calculate the total quantity sold and total revenue for each product using the cleaned transaction data (`df1_cleaned`), merge these results, sort the products, and display the top N products for each metric as per the instructions.


"""

# 1. Hitung jumlah total kuantitas terjual untuk setiap produk (berdasarkan StockCode) menggunakan df1_cleaned.
product_quantity = df1_cleaned.groupby('StockCode')['Quantity'].sum().reset_index()
product_quantity.rename(columns={'Quantity': 'TotalQuantitySold'}, inplace=True)

# 2. Hitung total pendapatan yang dihasilkan oleh setiap produk (berdasarkan StockCode) menggunakan df1_cleaned.
product_revenue = df1_cleaned.groupby('StockCode')['TotalPrice'].sum().reset_index()
product_revenue.rename(columns={'TotalPrice': 'TotalRevenue'}, inplace=True)

# 3. Gabungkan hasil dari langkah 1 dan 2 ke dalam DataFrame baru.
product_analysis_df = product_quantity.merge(product_revenue, on='StockCode')

# 4. Urutkan produk berdasarkan total kuantitas terjual dan total pendapatan secara menurun
top_selling_products = product_analysis_df.sort_values(by='TotalQuantitySold', ascending=False)
most_profitable_products = product_analysis_df.sort_values(by='TotalRevenue', ascending=False)

# 5. Tampilkan 10 produk teratas berdasarkan kuantitas terjual dan 10 produk teratas berdasarkan pendapatan.
print("10 Produk Teratas Berdasarkan Kuantitas Terjual:")
display(top_selling_products.head(10))

print("\n10 Produk Teratas Berdasarkan Pendapatan:")
display(most_profitable_products.head(10))

"""**Reasoning**:
Calculate the total quantity sold and total revenue for each product using the cleaned transaction data (`df1_cleaned`), merge these results, sort the products, and display the top N products for each metric as per the instructions.
"""

# 1. Hitung jumlah total kuantitas terjual untuk setiap produk (berdasarkan StockCode) menggunakan df1_cleaned.
product_quantity = df1_cleaned.groupby('StockCode')['Quantity'].sum().reset_index()
product_quantity.rename(columns={'Quantity': 'TotalQuantitySold'}, inplace=True)

# 2. Hitung total pendapatan yang dihasilkan oleh setiap produk (berdasarkan StockCode) menggunakan df1_cleaned.
product_revenue = df1_cleaned.groupby('StockCode')['TotalPrice'].sum().reset_index()
product_revenue.rename(columns={'TotalPrice': 'TotalRevenue'}, inplace=True)

# 3. Gabungkan hasil dari langkah 1 dan 2 ke dalam DataFrame baru.
product_analysis_df = product_quantity.merge(product_revenue, on='StockCode')

# 4. Urutkan produk berdasarkan total kuantitas terjual dan total pendapatan secara menurun
top_selling_products = product_analysis_df.sort_values(by='TotalQuantitySold', ascending=False)
most_profitable_products = product_analysis_df.sort_values(by='TotalRevenue', ascending=False)

# 5. Tampilkan 10 produk teratas berdasarkan kuantitas terjual dan 10 produk teratas berdasarkan pendapatan.
print("10 Produk Teratas Berdasarkan Kuantitas Terjual:")
display(top_selling_products.head(10))

print("\n10 Produk Teratas Berdasarkan Pendapatan:")
display(most_profitable_products.head(10))

"""**Reasoning**:
Identify churned customers based on the last transaction date in `df1_cleaned` and create a binary churn column in `rfm_df`. This addresses steps 1 and 2 of the subtask.


"""

# Step 1: Identify churned customers
# Define the observation period end date (the last date in the dataset)
observation_end_date = df1_cleaned['InvoiceDate'].max()

# Define the churn threshold (e.g., 90 days before the observation end date)
churn_threshold_date = observation_end_date - dt.timedelta(days=90)

# Get the last transaction date for each customer
last_transaction_date = df1_cleaned.groupby('CustomerID')['InvoiceDate'].max().reset_index()

# Identify churned customers (last transaction before the churn threshold date)
churned_customers = last_transaction_date[last_transaction_date['InvoiceDate'] < churn_threshold_date]

# Step 2: Create a binary column in rfm_df to mark churned customers
rfm_df['Churn'] = rfm_df['CustomerID'].apply(lambda x: 1 if x in churned_customers['CustomerID'].values else 0)

# Display the number of churned customers
print(f"Number of churned customers: {rfm_df['Churn'].sum()}")

# Display the first few rows of rfm_df with the new Churn column
display(rfm_df.head())

"""**Reasoning**:
The previous step successfully identified churned customers and added the 'Churn' column to `rfm_df`. Now, I need to select the relevant features from `rfm_df` to be used as input for the churn prediction model and define the target variable, which is the 'Churn' column. This addresses steps 3 and 4 of the subtask.


"""

# Step 3: Select relevant features from rfm_df for the churn prediction model
features = ['Recency', 'Frequency', 'Monetary', 'R_score', 'F_score', 'M_score', 'RFM_Score']
X = rfm_df[features]

# Step 4: Define the target variable (the new churn column)
y = rfm_df['Churn']

# Display the first few rows of the features and target
print("Features for churn prediction (X):")
display(X.head())

print("\nTarget variable for churn prediction (y):")
display(y.head())

"""**Reasoning**:
I need to select, train, and evaluate a classification model to predict customer churn. I will split the data, choose a model, train it, make predictions, and evaluate its performance using relevant metrics. This addresses steps 1 through 8 of the subtask.


"""

# Step 1 & 2: Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3 & 4: Import and initialize a classification model
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(random_state=42)

# Step 5: Train the model on the training data
model.fit(X_train, y_train)

# Step 6: Make predictions on the testing data
y_pred = model.predict(X_test)

# Step 7 & 8: Import and evaluate the model using relevant metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print(f"Model Accuracy: {accuracy:.4f}")
print(f"Model Precision: {precision:.4f}")
print(f"Model Recall: {recall:.4f}")
print(f"Model F1-Score: {f1:.4f}")
print(f"Model ROC AUC Score: {roc_auc:.4f}")

"""**Reasoning**:
Calculate CLV as the total monetary value, merge rfm_df with country information from df1_cleaned, select relevant features for the CLV prediction model, define the target variable, and display the head of the feature and target dataframes. This addresses steps 1-5 of the subtask.


"""

# Step 1: Calculate Customer Lifetime Value (CLV) as Total Monetary Value
rfm_df['CLV'] = rfm_df['Monetary']

# Step 2: Merge rfm_df with country information from df1_cleaned
# We need to get the country for each customer. Since a customer might have multiple countries
# in the transaction data (though less likely for a single customer ID), we'll take the most
# frequent country for each customer from df1_cleaned.
customer_country = df1_cleaned.groupby('CustomerID')['Country'].agg(lambda x: x.mode()[0]).reset_index()

# Merge country information into rfm_df
rfm_df_clv = rfm_df.merge(customer_country, on='CustomerID', how='left')

# Step 3: Select relevant features for the CLV prediction model
# Using RFM features and possibly Country (will need encoding for Country)
features_clv = ['Recency', 'Frequency', 'Monetary', 'R_score', 'F_score', 'M_score', 'RFM_Score']
X_clv = rfm_df_clv[features_clv]

# Step 4: Define the target variable (CLV)
y_clv = rfm_df_clv['CLV']

# Step 5: Display the first few rows of the feature and target dataframes
print("Features for CLV prediction (X_clv):")
display(X_clv.head())

print("\nTarget variable for CLV prediction (y_clv):")
display(y_clv.head())

"""**Reasoning**:
Split the data, train the Linear Regression model, make predictions, and evaluate the model using MAE, MSE, and R2 score. This addresses steps 1-8 of the subtask.


"""

# 1. Split the X_clv and y_clv data into training and testing sets
from sklearn.model_selection import train_test_split
X_train_clv, X_test_clv, y_train_clv, y_test_clv = train_test_split(X_clv, y_clv, test_size=0.2, random_state=42)

# 2. Import the LinearRegression model
from sklearn.linear_model import LinearRegression

# 3. Initialize a LinearRegression model
model_clv = LinearRegression()

# 4. Train the model using the training data
model_clv.fit(X_train_clv, y_train_clv)

# 5. Make predictions on the testing data
y_pred_clv = model_clv.predict(X_test_clv)

# 6. Import evaluation metrics for regression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 7. Calculate the MAE, MSE, and R2 score
mae = mean_absolute_error(y_test_clv, y_pred_clv)
mse = mean_squared_error(y_test_clv, y_pred_clv)
r2 = r2_score(y_test_clv, y_pred_clv)

# 8. Print the calculated evaluation metrics
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"R2 Score: {r2:.4f}")

"""**Reasoning**:
I will create a bar chart to visualize the distribution of customer segments using the counts calculated from `rfm_df['Segment']`. This addresses steps 2 through 7 of the subtask.


"""

# Calculate the counts of customers in each segment
segment_counts = rfm_df['Segment'].value_counts()

# Create a bar chart to visualize the distribution of customer segments
plt.figure(figsize=(10, 6))
sns.barplot(x=segment_counts.index, y=segment_counts.values)

# Add title and labels
plt.title("Distribution of Customer Segments")
plt.xlabel("Customer Segment")
plt.ylabel("Number of Customers")

# Rotate x-axis labels if necessary
plt.xticks(rotation=45, ha='right')

# Display the plot
plt.tight_layout()
plt.show()

"""**Reasoning**:
The first part of the visualization subtask, showing the distribution of customer segments, is complete. The next part is to visualize the top 10 products by quantity and revenue. This addresses steps 1 through 7 of the remaining subtask.


"""

# Create bar charts for top 10 products by quantity and revenue

# Top 10 Products by Quantity Sold
plt.figure(figsize=(12, 6))
sns.barplot(x='StockCode', y='TotalQuantitySold', data=top_selling_products.head(10), palette='viridis')
plt.title('Top 10 Products by Quantity Sold')
plt.xlabel('Product Stock Code')
plt.ylabel('Total Quantity Sold')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Top 10 Products by Total Revenue
plt.figure(figsize=(12, 6))
sns.barplot(x='StockCode', y='TotalRevenue', data=most_profitable_products.head(10), palette='viridis')
plt.title('Top 10 Products by Total Revenue')
plt.xlabel('Product Stock Code')
plt.ylabel('Total Revenue')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""**Reasoning**:
I will identify the top customer segment based on counts, filter the RFM data for this segment, calculate descriptive statistics for RFM metrics for this segment, merge the RFM data with the cleaned transaction data, and then find the top products and countries for this segment. This addresses steps 1-10 of the subtask.


"""

# Step 1: Identify the top customer segment
top_segment = rfm_df['Segment'].value_counts().index[0]
print(f"Top Customer Segment: {top_segment}")

# Step 2 & 3: Filter rfm_df for the top segment
top_segment_df = rfm_df[rfm_df['Segment'] == top_segment]

# Step 4 & 5: Calculate and display descriptive statistics for RFM metrics for the top segment
print(f"\nDescriptive Statistics for {top_segment} Segment:")
display(top_segment_df[['Recency', 'Frequency', 'Monetary']].describe())

# Step 6: Merge rfm_df with df1_cleaned to analyze purchasing behavior
# Need to ensure CustomerID is of the same type in both dataframes before merging
df1_cleaned['CustomerID'] = df1_cleaned['CustomerID'].astype(int)
rfm_df['CustomerID'] = rfm_df['CustomerID'].astype(int)

merged_df = pd.merge(df1_cleaned, rfm_df, on='CustomerID', how='inner')

# Filter the merged data for the top segment
top_segment_merged_df = merged_df[merged_df['Segment'] == top_segment]

# Step 7 & 8: Identify and display top 10 products for the top segment
print(f"\nTop 10 Products Purchased by {top_segment} Segment:")
top_segment_products = top_segment_merged_df['Description'].value_counts().head(10)
display(top_segment_products)

# Step 9 & 10: Identify and display the distribution of countries for the top segment
print(f"\nCountry Distribution for {top_segment} Segment:")
top_segment_countries = top_segment_merged_df['Country'].value_counts()
display(top_segment_countries)

"""**Reasoning**:
Identify products appearing in both top 10 lists, merge their quantity and revenue information, and display the combined data. This addresses steps 1, 2, and 3 of the subtask.


"""

# Step 1: Identify products appearing in both top 10 lists
# Get the StockCodes from the top 10 selling and top 10 profitable products
top_selling_stock_codes = top_selling_products.head(10)['StockCode']
most_profitable_stock_codes = most_profitable_products.head(10)['StockCode']

# Find the common StockCodes
common_stock_codes = list(set(top_selling_stock_codes) & set(most_profitable_stock_codes))

print("Products appearing in both Top 10 Selling and Top 10 Profitable lists:")
print(common_stock_codes)

# Step 2: Merge information for these common products
# Filter the original product_analysis_df for the common stock codes
common_products_analysis = product_analysis_df[product_analysis_df['StockCode'].isin(common_stock_codes)]

# Step 3: Display the DataFrame with combined information
print("\nCombined Analysis for Products in Both Top 10 Lists:")
display(common_products_analysis)

"""**Reasoning**:
Analyze and interpret the findings from the common products identified in the previous step to understand their performance across both quantity and revenue metrics. This addresses step 4 of the subtask and completes the subtask.


"""

# Step 4: Analyze and interpret the findings from these common products

print("Analysis and Interpretation of Common Top Products:")

# Sort the common products by TotalQuantitySold and TotalRevenue for easier comparison
common_products_analysis_sorted_qty = common_products_analysis.sort_values(by='TotalQuantitySold', ascending=False)
common_products_analysis_sorted_revenue = common_products_analysis.sort_values(by='TotalRevenue', ascending=False)

print("\nSorted by Total Quantity Sold:")
display(common_products_analysis_sorted_qty)

print("\nSorted by Total Revenue:")
display(common_products_analysis_sorted_revenue)

print("\nInterpretation:")
print(f"- There are {len(common_stock_codes)} products that are in the top 10 for both quantity sold and revenue generated.")
print("These products are likely key drivers of overall sales volume and profitability.")
print("They are consistently popular and valuable to the business.")
print("Further investigation into these products (e.g., marketing efforts, pricing, customer reviews) could provide insights into their success factors.")
print("The product with StockCode '85123A' stands out as the top performer in terms of Total Revenue, while '84879' has the highest Total Quantity Sold among these common products.")

"""**Reasoning**:
Analyze and interpret the findings from the common products identified in the previous step to understand their performance across both quantity and revenue metrics. This addresses step 4 of the subtask and completes the subtask.
"""

# Step 4: Analyze and interpret the findings from these common products

print("Analysis and Interpretation of Common Top Products:")

# Sort the common products by TotalQuantitySold and TotalRevenue for easier comparison
common_products_analysis_sorted_qty = common_products_analysis.sort_values(by='TotalQuantitySold', ascending=False)
common_products_analysis_sorted_revenue = common_products_analysis.sort_values(by='TotalRevenue', ascending=False)

print("\nSorted by Total Quantity Sold:")
display(common_products_analysis_sorted_qty)

print("\nSorted by Total Revenue:")
display(common_products_analysis_sorted_revenue)

print("\nInterpretation:")
print(f"- There are {len(common_stock_codes)} products that are in the top 10 for both quantity sold and revenue generated.")
print("These products are likely key drivers of overall sales volume and profitability.")
print("They are consistently popular and valuable to the business.")
print("Further investigation into these products (e.g., marketing efforts, pricing, customer reviews) could provide insights into their success factors.")
print("The product with StockCode '85123A' stands out as the top performer in terms of Total Revenue, while '84879' has the highest Total Quantity Sold among these common products.")

# Display the common top products sorted by quantity sold
print("Common Top Products Sorted by Total Quantity Sold:")
display(common_products_analysis_sorted_qty)

# Display the common top products sorted by revenue
print("Common Top Products Sorted by Total Revenue:")
display(common_products_analysis_sorted_revenue)